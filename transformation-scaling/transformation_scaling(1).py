# -*- coding: utf-8 -*-
"""transformation-scaling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UMGkLmnQDYTbjPttdeSEk9-iECsPRK-v
"""

import pandas as pd

cars = pd.read_csv('/content/mtcars_mod.csv')

cars

"""#1. Which scaling method works best for data with outliers?

##The best scaling method for handling outliers is robust scaling.

#2. Which scaling method produces data that is normally distributed? What is its mean and variance?

##MinMax scaling method

#3.  Which scaling method does not remove sparsity? What is sparsity?
Standard scaling does not remove sparsity because standard scaling changes data to be between 0 and 1, and sparse data would make this difficult to attain. Sparsity is known as scattered data.

#4.  Which scaling method is best to use if the bounds of your data are known from domain knowledge?
Standard scaler works well for the bounds of data being known. This is because standard scalers are useful for features that are distributed evenly and normally.

#1. Use the most appropriate scaling technique for each variable to scale disp, hp, drat, and wt, assuming wt should be normally distributed and the true bounds are known for disp.
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler

cars.disp.describe()

disp = cars[['disp']]

scaler_minmax = MinMaxScaler()
scaler_minmax.fit(disp)
scaled_array = scaler_minmax.transform(disp)
#scaled_array = scaler_minmax.fit_transform(new)
scaled_array

import numpy as np

disp_minmax = disp.copy()
disp_minmax.loc[:, ['disp']] = scaled_array

disp_minmax.hist()

hp = cars[['hp']]

scaler_standard = StandardScaler()
scaler_standard.fit(hp)
standard_array = scaler_standard.transform(hp)
hp_standard = hp.copy()
hp_standard.loc[:, ['hp']] = standard_array

hp_standard.hist()

cars.drat.describe()

drat = cars[['drat']]

scaler_rob = RobustScaler()
scaler_rob.fit(drat)
rob_array = scaler_rob.transform(drat)
drat_rob = drat.copy()
drat_rob.loc[:, ['drat']] = rob_array

drat_rob.describe()

wt = cars[['wt']]

scaler_minmax = MinMaxScaler()
scaler_minmax.fit(wt)
scaled_array = scaler_minmax.transform(wt)
#scaled_array = scaler_minmax.fit_transform(new)
scaled_array

"""#2. Use Box-Cox or Yeo-Johnson to determine the best transformation to use to make mpg and qsec normally distributed. Show the transformed data and the value of $\lambda$."""

from scipy.stats import boxcox, yeojohnson

mpg_nona = cars['mpg'].isna()== False
mpg_nona.describe()

box_mpg = boxcox(cars['mpg'])
box_mpg

import matplotlib.pyplot as plt

mpg = cars  ['mpg']

fig, ax = plt.subplots(1, 2)
ax[0].hist(mpg)
ax[1].hist(box_mpg)
fig.show()

mpg_yj = yeojohnson(mpg)
mpg_yj

fig, ax = plt.subplots(1, 2)
ax[0].hist(mpg)
ax[1].hist(mpg_yj, bins = 10)
fig.show()

"""Box-Cox method shows more normally distributed data for mpg."""



cars['qsec'] = cars['qsec'].apply(lambda x: str(x).replace('-', '') if x < 0 else x)

cars['qsec'] = pd.to_numeric(cars['qsec'])

box_qsec = (cars['qsec'])
box_qsec

qsec = cars['qsec']

fig, ax = plt.subplots(1, 2)
ax[0].hist(qsec)
ax[1].hist(box_qsec)
fig.show()

qsec_yj = yeojohnson(qsec)
qsec_yj

fig, ax = plt.subplots(1, 2)
ax[0].hist(qsec)
ax[1].hist(qsec_yj)
fig.show()

"""## The boxcox method shows a more normal distribution for the qsec column.

#3. Normalize all of the numeric rows using L2-normalization and display the new dataframe.
"""

cars.info()

from sklearn.preprocessing import Normalizer

cars_num = cars['mpg'], cars['cyl'], cars['disp'], cars['hp'], cars['drat'], cars['wt'], cars['qsec'], cars['vs'], 	cars['am'], 	cars['gear'], 	cars['carb'], 	cars['qsex']

cars_norm = Normalizer(norm = 'l2').fit(cars_num)
cars_norm = cars_norm.transform(cars_num)
cars_norm